{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#ola-eu-sou-o-andre","title":"Ol\u00e1, eu sou o Andr\u00e9.","text":"<p>Bem-vindo ao meu portf\u00f3lio t\u00e9cnico, criei este espa\u00e7o com a inten\u00e7\u00e3o de centralizar e documentar meus projetos. A ideia \u00e9 compartilhar o que estou estudando e construindo.</p> <p>Sou formado em Desenvolvimento de Sistemas e estou cursando Ci\u00eancia de Dados e Neg\u00f3cios. E por isso, este ser\u00e1 um espa\u00e7o com os mais diversos estudos e experimentos relacionados a tecnologia, dados, sistemas, estat\u00edstica e matem\u00e1tica.</p> <p></p>"},{"location":"#explorar","title":"Explorar","text":"<p>Ir para a \u00e1rea de Machine Learning</p> <p>Aviso</p> <p>O portf\u00f3lio acabou de nascer. Estou trazendo meus projetos e anota\u00e7\u00f5es para c\u00e1 aos poucos, ent\u00e3o voc\u00ea ver\u00e1 poucas p\u00e1ginas por enquanto.</p> <p></p>"},{"location":"#contato","title":"Contato","text":"<ul> <li> LinkedIn<ul> <li>linkedin.com/in/andre-alves777</li> </ul> </li> <li> GitHub<ul> <li>github.com/andre-alves77</li> </ul> </li> <li> Email<ul> <li>andre.alves68@outlook.com</li> </ul> </li> <li> Telefone/Whatsapp<ul> <li>+55 (11) 91429-1819</li> </ul> </li> </ul>"},{"location":"ml/","title":"Machine Learning","text":""},{"location":"ml/#machine-learning","title":"Machine Learning","text":"<p>Esta \u00e1rea cont\u00e9m estudos relacionados ao Aprendizado de M\u00e1quina, um campo de estudo que une matem\u00e1tica, estat\u00edstica e programa\u00e7\u00e3o para permitir que m\u00e1quinas aprendam com dados e/ou realizem tarefas de forma aut\u00f4noma.</p> <p>Inicialmente, utilizarei bibliotecas prontas para realizar a implementa\u00e7\u00e3o dos algoritmos de ML. Mais \u00e0 frente, quero implementar os algoritmos manualmente.</p>"},{"location":"ml/#estudos-publicados","title":"Estudos Publicados","text":""},{"location":"ml/#analise-de-varejo-online-online-retail","title":"An\u00e1lise de Varejo Online (Online Retail)","text":"<p>Este \u00e9 um estudo de caso cl\u00e1ssico de dados transacionais do UC Irvine Machine Learning Repository</p> <p> T\u00f3picos abordados: * Limpeza de dados (Data Cleaning) * An\u00e1lise Explorat\u00f3ria (EDA) * Segmenta\u00e7\u00e3o de Clientes (RFM, Clustering)</p>"},{"location":"ml/notebook_utils/","title":"Notebook utils","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nfrom pandas.api.types import is_numeric_dtype, is_datetime64_any_dtype\nfrom IPython.display import display, HTML, Markdown\n</pre> import pandas as pd from pandas.api.types import is_numeric_dtype, is_datetime64_any_dtype from IPython.display import display, HTML, Markdown In\u00a0[\u00a0]: Copied! <pre># CSS Centralizado\nGLOBAL_STYLE = \"\"\"\n&lt;style&gt;\n    /* 1. CONTAINER DA C\u00c9LULA */\n    div.cell_output {\n        overflow-x: auto;\n        -webkit-overflow-scrolling: touch;\n    }\n    div.cell_output::-webkit-scrollbar { height: 8px; }\n    div.cell_output::-webkit-scrollbar-thumb { background-color: #ccc; border-radius: 4px; }\n    div.cell_output::-webkit-scrollbar-track { background-color: #f1f1f1; }\n\n    /* 2. ESTILO DAS TABELAS */\n    table.dataframe, .style-wrap table {\n        border-collapse: collapse;\n        border: 1px solid #ccc;\n        width: 100%;\n        margin-bottom: 20px;\n    }\n    table.dataframe th, .style-wrap th {\n        background-color: #f2f2f2;\n        color: #333;\n        font-weight: bold;\n        padding: 10px;\n        border-bottom: 2px solid #aaa;\n        text-align: left;\n    }\n    table.dataframe td, .style-wrap td {\n        padding: 8px;\n        border: 1px solid #ddd;\n    }\n    table.dataframe tr:nth-child(even), .style-wrap tr:nth-child(even) {\n        background-color: #f9f9f9;\n    }\n&lt;/style&gt;\n\"\"\"\n</pre> # CSS Centralizado GLOBAL_STYLE = \"\"\"  \"\"\" In\u00a0[\u00a0]: Copied! <pre>def setup_notebook():\n    \"\"\"Configura\u00e7\u00f5es iniciais do notebook e inje\u00e7\u00e3o de CSS.\"\"\"\n    pd.set_option('display.max_columns', None)\n    display(HTML(GLOBAL_STYLE))\n</pre> def setup_notebook():     \"\"\"Configura\u00e7\u00f5es iniciais do notebook e inje\u00e7\u00e3o de CSS.\"\"\"     pd.set_option('display.max_columns', None)     display(HTML(GLOBAL_STYLE)) In\u00a0[\u00a0]: Copied! <pre>def format_table(df: pd.DataFrame, currency_cols: list = None, date_cols: list = None, hide_index: bool = True):\n    \"\"\"\n    Aplica formata\u00e7\u00e3o padr\u00e3o do projeto aos DataFrames de forma segura.\n    \"\"\"\n    style = df.style\n    format_dict = {}\n    \n    # 1. Formata\u00e7\u00e3o de Moeda (S\u00f3 aplica se for num\u00e9rico)\n    if currency_cols:\n        for col in currency_cols:\n            if col in df.columns:\n                if is_numeric_dtype(df[col]):\n                    format_dict[col] = \"\u00a3 {:.2f}\"\n                else:\n                    # Opcional: Avisar ou tentar converter silenciosamente\n                    # print(f\"Aviso: Coluna '{col}' n\u00e3o \u00e9 num\u00e9rica. Formata\u00e7\u00e3o ignorada.\")\n                    pass\n\n    # 2. Formata\u00e7\u00e3o de Data (S\u00f3 aplica se for datetime)\n    if date_cols:\n        for col in date_cols:\n            if col in df.columns:\n                if is_datetime64_any_dtype(df[col]):\n                    format_dict[col] = \"{:%d/%m/%Y %H:%M}\"\n                else:\n                    # Se n\u00e3o for datetime, n\u00e3o tenta formatar com %d/%m...\n                    # print(f\"Aviso: Coluna '{col}' n\u00e3o \u00e9 datetime. Formata\u00e7\u00e3o ignorada.\")\n                    pass\n\n    if format_dict:\n        style = style.format(format_dict)\n\n    if hide_index:\n        style = style.hide(axis=\"index\")\n        \n    return style\n</pre> def format_table(df: pd.DataFrame, currency_cols: list = None, date_cols: list = None, hide_index: bool = True):     \"\"\"     Aplica formata\u00e7\u00e3o padr\u00e3o do projeto aos DataFrames de forma segura.     \"\"\"     style = df.style     format_dict = {}          # 1. Formata\u00e7\u00e3o de Moeda (S\u00f3 aplica se for num\u00e9rico)     if currency_cols:         for col in currency_cols:             if col in df.columns:                 if is_numeric_dtype(df[col]):                     format_dict[col] = \"\u00a3 {:.2f}\"                 else:                     # Opcional: Avisar ou tentar converter silenciosamente                     # print(f\"Aviso: Coluna '{col}' n\u00e3o \u00e9 num\u00e9rica. Formata\u00e7\u00e3o ignorada.\")                     pass      # 2. Formata\u00e7\u00e3o de Data (S\u00f3 aplica se for datetime)     if date_cols:         for col in date_cols:             if col in df.columns:                 if is_datetime64_any_dtype(df[col]):                     format_dict[col] = \"{:%d/%m/%Y %H:%M}\"                 else:                     # Se n\u00e3o for datetime, n\u00e3o tenta formatar com %d/%m...                     # print(f\"Aviso: Coluna '{col}' n\u00e3o \u00e9 datetime. Formata\u00e7\u00e3o ignorada.\")                     pass      if format_dict:         style = style.format(format_dict)      if hide_index:         style = style.hide(axis=\"index\")              return style In\u00a0[\u00a0]: Copied! <pre>def display_missing_analysis(df: pd.DataFrame):\n    \"\"\"\n    Padroniza a exibi\u00e7\u00e3o da an\u00e1lise de valores ausentes.\n    \"\"\"\n    display(Markdown(\"**Total de valores ausentes por coluna:**\"))\n    \n    missing_df = df.isna().sum().to_frame(name='Total de Ausentes')\n    missing_df['% Ausente'] = (missing_df['Total de Ausentes'] / len(df)) * 100\n    missing_df_filtrado = missing_df[missing_df['Total de Ausentes'] &gt; 0].sort_values(by='Total de Ausentes', ascending=False)\n    \n    if missing_df_filtrado.empty:\n        display(Markdown(\"_Nenhum valor ausente encontrado._\"))\n    else:\n        estilo = missing_df_filtrado.style.format({\n            'Total de Ausentes': '{:,.0f}',\n            '% Ausente': '{:.2f}%'\n        })\n        display(estilo)\n</pre> def display_missing_analysis(df: pd.DataFrame):     \"\"\"     Padroniza a exibi\u00e7\u00e3o da an\u00e1lise de valores ausentes.     \"\"\"     display(Markdown(\"**Total de valores ausentes por coluna:**\"))          missing_df = df.isna().sum().to_frame(name='Total de Ausentes')     missing_df['% Ausente'] = (missing_df['Total de Ausentes'] / len(df)) * 100     missing_df_filtrado = missing_df[missing_df['Total de Ausentes'] &gt; 0].sort_values(by='Total de Ausentes', ascending=False)          if missing_df_filtrado.empty:         display(Markdown(\"_Nenhum valor ausente encontrado._\"))     else:         estilo = missing_df_filtrado.style.format({             'Total de Ausentes': '{:,.0f}',             '% Ausente': '{:.2f}%'         })         display(estilo)"},{"location":"ml/online-retail/1-eda/eda/","title":"An\u00e1lise Inicial","text":"In\u00a0[3]: Copied! <pre>display(Markdown(f\"**Dimens\u00e3o da base:** `{df.shape[0]:,}` linhas e `{df.shape[1]}` colunas.\"))\n</pre> display(Markdown(f\"**Dimens\u00e3o da base:** `{df.shape[0]:,}` linhas e `{df.shape[1]}` colunas.\")) <p>Dimens\u00e3o da base: <code>1,067,371</code> linhas e <code>8</code> colunas.</p> Variable Description InvoiceNo C\u00f3dido de cada transa\u00e7\u00e3o. Se come\u00e7ar com a letra 'c', indica cancelamento StockCode C\u00f3digo do produto Description Descri\u00e7\u00e3o do produto Quantity Quantidade de itens comprados InvoiceDate Data e hora da compra UnitPrice Pre\u00e7o unit\u00e1rio do produto CustomerID Identificador \u00fanico do cliente Country Pa\u00eds do cliente Invoice StockCode Description Quantity InvoiceDate Price Customer ID Country 489434 85048 15CM CHRISTMAS GLASS BALL 20 LIGHTS 12 01/12/2009 07:45 6.950000 13085.000000 United Kingdom 489434 79323P PINK CHERRY LIGHTS 12 01/12/2009 07:45 6.750000 13085.000000 United Kingdom 489434 79323W  WHITE CHERRY LIGHTS 12 01/12/2009 07:45 6.750000 13085.000000 United Kingdom 489434 22041 RECORD FRAME 7\" SINGLE SIZE  48 01/12/2009 07:45 2.100000 13085.000000 United Kingdom 489434 21232 STRAWBERRY CERAMIC TRINKET BOX 24 01/12/2009 07:45 1.250000 13085.000000 United Kingdom <p>Total de valores ausentes por coluna:</p> Total de Ausentes % Ausente Customer ID 243,007 22.77% Description 4,382 0.41% Quantity Price Customer ID count 1067371.00 1067371.00 824364.00 mean 9.94 4.65 15324.64 std 172.71 123.55 1697.46 min -80995.00 -53594.36 12346.00 25% 1.00 1.25 13975.00 50% 3.00 2.10 15255.00 75% 10.00 4.15 16797.00 max 80995.00 38970.00 18287.00 InvoiceDate count 1067371 mean 2011-01-02 21:13:55.394028544 min 2009-12-01 07:45:00 25% 2010-07-09 09:46:00 50% 2010-12-07 15:28:00 75% 2011-07-22 10:23:00 max 2011-12-09 12:50:00 In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"ml/online-retail/1-eda/eda/#analise-inicial","title":"An\u00e1lise Inicial\u00b6","text":"<p>O conjunto de dados utilizado neste projeto \u00e9 o Online Retail II, disponibilizado pelo UCI Machine Learning Repository.</p> <p>Cada linha representa uma transa\u00e7\u00e3o, ao todo o dataset possui 2 anos de transa\u00e7\u00f5es ocorridas ao longo de 2009-2011 numa empresa de varejo online do Reino Unido, especializada na venda de presentes e artigos dom\u00e9sticos.</p>"},{"location":"ml/online-retail/1-eda/eda/#descricao-da-base","title":"Descri\u00e7\u00e3o da base\u00b6","text":""},{"location":"ml/online-retail/1-eda/eda/#amostra-dos-dados","title":"Amostra dos dados\u00b6","text":""},{"location":"ml/online-retail/1-eda/eda/#qualidade-dos-dados","title":"Qualidade dos dados\u00b6","text":""},{"location":"ml/online-retail/1-eda/eda/#estatisticas-descritivas-basicas","title":"Estat\u00edsticas descritivas b\u00e1sicas\u00b6","text":""},{"location":"ml/online-retail/1-eda/eda/#analises-agregadas-iniciais","title":"An\u00e1lises agregadas iniciais\u00b6","text":""},{"location":"ml/online-retail/1-eda/eda/#analise-de-correlacao","title":"An\u00e1lise de correla\u00e7\u00e3o\u00b6","text":""},{"location":"ml/online-retail/2-tree/tree/","title":"Decision Tree","text":"In\u00a0[93]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\n\n%matplotlib inline\n\nimport os\nimport sys\nsys.path.append(os.path.abspath(\"../..\"))\nimport notebook_utils as nu\nfrom IPython.display import display, Markdown as display_markdown\n\nnu.setup_notebook()\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier, plot_tree from sklearn.metrics import accuracy_score, classification_report, confusion_matrix from sklearn.preprocessing import StandardScaler, LabelEncoder   %matplotlib inline  import os import sys sys.path.append(os.path.abspath(\"../..\")) import notebook_utils as nu from IPython.display import display, Markdown as display_markdown  nu.setup_notebook() In\u00a0[94]: Copied! <pre>df = pd.read_csv('../assets/data.csv', encoding='ISO-8859-1')\n</pre> df = pd.read_csv('../assets/data.csv', encoding='ISO-8859-1') <ul> <li><p>Limpeza: Remo\u00e7\u00e3o das linhas sem CustomerID para focar em clientes identific\u00e1veis.</p> </li> <li><p>Target (Is_Cancelled): Cria\u00e7\u00e3o da coluna alvo identificando transa\u00e7\u00f5es que come\u00e7am com 'C' no InvoiceNo.</p> </li> </ul> In\u00a0[95]: Copied! <pre>df_clean = df.copy()\n\ndf_clean = df_clean.dropna(subset=['CustomerID'])\n\ndf_clean['Is_Cancelled'] = df_clean['InvoiceNo'].astype(str).str.upper().str.startswith('C').astype(int)\n\n# Feature Engineering\ndf_clean['InvoiceDate'] = pd.to_datetime(df_clean['InvoiceDate'])\ndf_clean['Month'] = df_clean['InvoiceDate'].dt.month\ndf_clean['Hour'] = df_clean['InvoiceDate'].dt.hour\n\ndf_clean['Quantity_Abs'] = df_clean['Quantity'].abs()\ndf_clean['TotalAmount'] = df_clean['Quantity_Abs'] * df_clean['UnitPrice']\n\nle = LabelEncoder()\ndf_clean['Country_Encoded'] = le.fit_transform(df_clean['Country'])\n\nfeatures = ['UnitPrice', 'Quantity_Abs', 'TotalAmount', 'Month', 'Hour', 'Country_Encoded']\nX = df_clean[features]\ny = df_clean['Is_Cancelled']\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX = pd.DataFrame(X_scaled, columns=features)\n\nprint(f\"Distribui\u00e7\u00e3o de Cancelamentos:\\n{y.value_counts(normalize=True)}\")\n</pre> df_clean = df.copy()  df_clean = df_clean.dropna(subset=['CustomerID'])  df_clean['Is_Cancelled'] = df_clean['InvoiceNo'].astype(str).str.upper().str.startswith('C').astype(int)  # Feature Engineering df_clean['InvoiceDate'] = pd.to_datetime(df_clean['InvoiceDate']) df_clean['Month'] = df_clean['InvoiceDate'].dt.month df_clean['Hour'] = df_clean['InvoiceDate'].dt.hour  df_clean['Quantity_Abs'] = df_clean['Quantity'].abs() df_clean['TotalAmount'] = df_clean['Quantity_Abs'] * df_clean['UnitPrice']  le = LabelEncoder() df_clean['Country_Encoded'] = le.fit_transform(df_clean['Country'])  features = ['UnitPrice', 'Quantity_Abs', 'TotalAmount', 'Month', 'Hour', 'Country_Encoded'] X = df_clean[features] y = df_clean['Is_Cancelled']  scaler = StandardScaler() X_scaled = scaler.fit_transform(X) X = pd.DataFrame(X_scaled, columns=features)  print(f\"Distribui\u00e7\u00e3o de Cancelamentos:\\n{y.value_counts(normalize=True)}\") <pre>Distribui\u00e7\u00e3o de Cancelamentos:\nIs_Cancelled\n0    0.978111\n1    0.021889\nName: proportion, dtype: float64\n</pre> <p>Uso do stratify=y porque cancelamentos s\u00e3o eventos raros (classe desbalanceada) e stratify garante que a propor\u00e7\u00e3o de cancelamentos seja a mesma no treino e no teste.</p> In\u00a0[96]: Copied! <pre># Divis\u00e3o 70% Treino / 30% Teste\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n</pre> # Divis\u00e3o 70% Treino / 30% Teste X_train, X_test, y_train, y_test = train_test_split(     X, y, test_size=0.3, random_state=42, stratify=y )  <pre>Dados de Treino: (284780, 6)\nDados de Teste: (122049, 6)\n</pre> <p>class_weight='balanced': Obriga o modelo a prestar aten\u00e7\u00e3o na classe minorit\u00e1ria (os cancelamentos). Sem isso, ele poderia ignor\u00e1-los para maximizar a acur\u00e1cia geral.</p> <p>max_depth=4: Mant\u00e9m a \u00e1rvore simples e interpret\u00e1vel, evitando overfitting.</p> In\u00a0[98]: Copied! <pre>clf = DecisionTreeClassifier(\n    criterion='gini',\n    max_depth=4,\n    random_state=42,\n    class_weight='balanced')\n\nclf.fit(X_train, y_train)\n</pre> clf = DecisionTreeClassifier(     criterion='gini',     max_depth=4,     random_state=42,     class_weight='balanced')  clf.fit(X_train, y_train) Out[98]: <pre>DecisionTreeClassifier(class_weight='balanced', max_depth=4, random_state=42)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFitted Parameters criterion\u00a0 'gini' splitter\u00a0 'best' max_depth\u00a0 4 min_samples_split\u00a0 2 min_samples_leaf\u00a0 1 min_weight_fraction_leaf\u00a0 0.0 max_features\u00a0 None random_state\u00a0 42 max_leaf_nodes\u00a0 None min_impurity_decrease\u00a0 0.0 class_weight\u00a0 'balanced' ccp_alpha\u00a0 0.0 monotonic_cst\u00a0 None In\u00a0[99]: Copied! <pre>y_pred = clf.predict(X_test)\n</pre> y_pred = clf.predict(X_test) In\u00a0[100]: Copied! <pre>print(classification_report(y_test, y_pred, target_names=['Venda Normal', 'Cancelamento']))\n</pre> print(classification_report(y_test, y_pred, target_names=['Venda Normal', 'Cancelamento'])) <pre>              precision    recall  f1-score   support\n\nVenda Normal       0.99      0.74      0.84    119377\nCancelamento       0.05      0.59      0.09      2672\n\n    accuracy                           0.73    122049\n   macro avg       0.52      0.67      0.47    122049\nweighted avg       0.97      0.73      0.83    122049\n\n</pre> In\u00a0[101]: Copied! <pre>plt.figure(figsize=(6, 5))\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Reds', xticklabels=['Venda', 'Cancelado'], yticklabels=['Venda', 'Cancelado'])\nplt.ylabel('Real')\nplt.xlabel('Predito')\nplt.show()\n</pre> plt.figure(figsize=(6, 5)) cm = confusion_matrix(y_test, y_pred) sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', xticklabels=['Venda', 'Cancelado'], yticklabels=['Venda', 'Cancelado']) plt.ylabel('Real') plt.xlabel('Predito') plt.show()  In\u00a0[102]: Copied! <pre>plt.figure(figsize=(25, 10))\nplot_tree(clf, feature_names=features, class_names=['Venda', 'Cancelado'], filled=True, fontsize=10)\nplt.title(\"\u00c1rvore de Decis\u00e3o - Risco de Cancelamento\")\nplt.show()\n</pre> plt.figure(figsize=(25, 10)) plot_tree(clf, feature_names=features, class_names=['Venda', 'Cancelado'], filled=True, fontsize=10) plt.title(\"\u00c1rvore de Decis\u00e3o - Risco de Cancelamento\") plt.show() In\u00a0[104]: Copied! <pre>importances = pd.Series(clf.feature_importances_, index=features).sort_values(ascending=False)\nprint(importances)\n</pre> importances = pd.Series(clf.feature_importances_, index=features).sort_values(ascending=False) print(importances) <pre>Quantity_Abs       0.545769\nHour               0.202847\nTotalAmount        0.108878\nCountry_Encoded    0.097681\nMonth              0.035014\nUnitPrice          0.009811\ndtype: float64\n</pre> <p>Acur\u00e1cia (73.4%): Embora inferior a um modelo base (que teria ~98% apenas prevendo a classe majorit\u00e1ria), esta m\u00e9trica reflete a decis\u00e3o de priorizar a detec\u00e7\u00e3o da classe minorit\u00e1ria.</p> <p>Recall (Sensibilidade) da Classe 'Cancelamento' (0.59): O modelo foi capaz de identificar corretamente 59% de todas as transa\u00e7\u00f5es que resultariam em cancelamento. Para um problema de detec\u00e7\u00e3o de risco/fraude, o Recall \u00e9 frequentemente a m\u00e9trica mais cr\u00edtica.</p> <p>Precision (Precis\u00e3o) da Classe 'Cancelamento' (0.05): O modelo apresentou uma alta taxa de Falsos Positivos. Isso indica que, para garantir que os cancelamentos fossem detectados, o modelo se tornou \"agressivo\", classificando muitas vendas normais como risco.</p>"},{"location":"ml/online-retail/2-tree/tree/#decision-tree","title":"Decision Tree\u00b6","text":"<p>Estudo inicial do comportamento do algoritimo de Arvore de Decis\u00e3o com essa base de dados.</p>"},{"location":"ml/online-retail/2-tree/tree/#target","title":"Target\u00b6","text":"<p>Como target inicial, utilizarei a flag 'Is_Internacional' que indicar\u00e1 se uma compra foi feita Reino Unido ou fora dele.</p> <p>Esse target n\u00e3o tem um impacto direto em um problema de neg\u00f3cio, por\u00e9m, ao aplicar a \u00c0rvore de Decis\u00e3o voltada a analisar compras internacionais, poderemos obter novas perspectivas sobre o comportamento dos clientes.</p>"},{"location":"ml/online-retail/2-tree/tree/#importacao-de-bibliotecas","title":"Importa\u00e7\u00e3o de Bibliotecas\u00b6","text":""},{"location":"ml/online-retail/2-tree/tree/#carga","title":"Carga\u00b6","text":""},{"location":"ml/online-retail/2-tree/tree/#pre-processamento","title":"Pr\u00e9-processamento\u00b6","text":""},{"location":"ml/online-retail/2-tree/tree/#engenharia-de-features","title":"Engenharia de Features:\u00b6","text":"<ul> <li><p>Convers\u00e3o de datas para extrair m\u00eas e hora.</p> </li> <li><p>Corre\u00e7\u00e3o de Vazamento: Cria\u00e7\u00e3o de Quantity_Abs. O modelo precisa saber que \"comprar 5 itens\" tem risco de cancelamento. Se continuasse \"-5\", ele aprenderia o sinal negativo, o que \u00e9 in\u00fatil para prever risco futuro.</p> </li> <li><p>Normaliza\u00e7\u00e3o dos dados num\u00e9ricos para atender \u00e0 rubrica.</p> </li> </ul>"},{"location":"ml/online-retail/2-tree/tree/#divisao-treino-e-teste-dos-dados","title":"Divis\u00e3o treino e teste dos dados\u00b6","text":""},{"location":"ml/online-retail/2-tree/tree/#treinamento-do-modelo","title":"Treinamento do modelo\u00b6","text":""},{"location":"ml/online-retail/2-tree/tree/#teste-e-avaliacao","title":"Teste e Avalia\u00e7\u00e3o\u00b6","text":""},{"location":"ml/online-retail/2-tree/tree/#predicao","title":"Predi\u00e7\u00e3o\u00b6","text":""},{"location":"ml/online-retail/2-tree/tree/#metricas","title":"M\u00e9tricas\u00b6","text":""},{"location":"ml/online-retail/2-tree/tree/#matriz-de-confusao","title":"Matriz de Confus\u00e3o\u00b6","text":""},{"location":"ml/online-retail/2-tree/tree/#visualizacao-da-arvore","title":"Visualiza\u00e7\u00e3o da \u00c1rvore\u00b6","text":""},{"location":"ml/online-retail/2-tree/tree/#importancia-das-features","title":"Import\u00e2ncia das Features\u00b6","text":""},{"location":"ml/online-retail/2-tree/tree/#analise-e-interpretacao","title":"An\u00e1lise e Interpreta\u00e7\u00e3o\u00b6","text":""},{"location":"ml/online-retail/2-tree/tree/#conclusao","title":"Conclus\u00e3o\u00b6","text":"<p>O modelo atua efetivamente como um filtro de triagem inicial. Embora gere muitos alarmes falsos (baixa precis\u00e3o), ele captura a maioria dos problemas reais (bom recall).</p>"},{"location":"ml/online-retail/3-knn/knn/","title":"KNN","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\n# Configura\u00e7\u00f5es visuais\nsns.set_style(\"whitegrid\")\npd.set_option('display.max_columns', None)\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score, classification_report, confusion_matrix from sklearn.preprocessing import StandardScaler, LabelEncoder  # Configura\u00e7\u00f5es visuais sns.set_style(\"whitegrid\") pd.set_option('display.max_columns', None) In\u00a0[2]: Copied! <pre>df = pd.read_csv('../assets/data.csv', encoding='ISO-8859-1')\n\ndf = df.dropna(subset=['CustomerID'])\n\ndf['Is_Cancelled'] = df['InvoiceNo'].astype(str).str.upper().str.startswith('C').astype(int)\ndf['Quantity_Abs'] = df['Quantity'].abs()\ndf['TotalAmount'] = df['Quantity_Abs'] * df['UnitPrice']\n</pre> df = pd.read_csv('../assets/data.csv', encoding='ISO-8859-1')  df = df.dropna(subset=['CustomerID'])  df['Is_Cancelled'] = df['InvoiceNo'].astype(str).str.upper().str.startswith('C').astype(int) df['Quantity_Abs'] = df['Quantity'].abs() df['TotalAmount'] = df['Quantity_Abs'] * df['UnitPrice']  In\u00a0[3]: Copied! <pre>sample_plot = df.sample(5000, random_state=42)\nplt.figure(figsize=(10, 6))\nsns.scatterplot(\n    data=sample_plot[sample_plot['TotalAmount'] &lt; 200], # Filtro visual para remover outliers extremos\n    x='UnitPrice', \n    y='Quantity_Abs', \n    hue='Is_Cancelled',\n    palette={0: 'blue', 1: 'red'},\n    alpha=0.6\n)\nplt.title('Distribui\u00e7\u00e3o Espacial: Pre\u00e7o vs Quantidade (Amostra)')\nplt.xlabel('Pre\u00e7o Unit\u00e1rio')\nplt.ylabel('Quantidade (Absoluta)')\nplt.show()\n</pre> sample_plot = df.sample(5000, random_state=42) plt.figure(figsize=(10, 6)) sns.scatterplot(     data=sample_plot[sample_plot['TotalAmount'] &lt; 200], # Filtro visual para remover outliers extremos     x='UnitPrice',      y='Quantity_Abs',      hue='Is_Cancelled',     palette={0: 'blue', 1: 'red'},     alpha=0.6 ) plt.title('Distribui\u00e7\u00e3o Espacial: Pre\u00e7o vs Quantidade (Amostra)') plt.xlabel('Pre\u00e7o Unit\u00e1rio') plt.ylabel('Quantidade (Absoluta)') plt.show()  In\u00a0[16]: Copied! <pre>df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\ndf['Hour'] = df['InvoiceDate'].dt.hour\ndf['Month'] = df['InvoiceDate'].dt.month\ndf['Country'] = LabelEncoder().fit_transform(df['Country'])\n</pre> df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate']) df['Hour'] = df['InvoiceDate'].dt.hour df['Month'] = df['InvoiceDate'].dt.month df['Country'] = LabelEncoder().fit_transform(df['Country'])  In\u00a0[17]: Copied! <pre>features = ['UnitPrice', 'Quantity_Abs', 'TotalAmount', 'Hour', 'Month', 'Country']\nX = df[features]\ny = df['Is_Cancelled']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n</pre> features = ['UnitPrice', 'Quantity_Abs', 'TotalAmount', 'Hour', 'Month', 'Country'] X = df[features] y = df['Is_Cancelled']  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)  In\u00a0[\u00a0]: Copied! <pre>scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nle = LabelEncoder()\ndf['Country'] = le.fit_transform(df['Country'])\n</pre> scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test)  le = LabelEncoder() df['Country'] = le.fit_transform(df['Country']) In\u00a0[27]: Copied! <pre>error_rates = []\n# Testando apenas K \u00edmpares para evitar empates\nk_values = range(1, 20, 2) \n\nfor k in k_values:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    pred_i = knn.predict(X_test)\n    # Salvando a taxa de erro (1 - Acur\u00e1cia)\n    error_rates.append(np.mean(pred_i != y_test))\n    print(f\"K={k}: Erro={error_rates[-1]:.4f}\")\n\n# Plot do Cotovelo\nplt.figure(figsize=(10, 6))\nplt.plot(k_values, error_rates, color='blue', linestyle='dashed', marker='o', markerfacecolor='red', markersize=10)\nplt.title('Taxa de Erro vs. Valor de K')\nplt.xlabel('K (N\u00famero de Vizinhos)')\nplt.ylabel('Taxa de Erro')\nplt.xticks(k_values)\nplt.show()\n</pre> error_rates = [] # Testando apenas K \u00edmpares para evitar empates k_values = range(1, 20, 2)   for k in k_values:     knn = KNeighborsClassifier(n_neighbors=k)     knn.fit(X_train, y_train)     pred_i = knn.predict(X_test)     # Salvando a taxa de erro (1 - Acur\u00e1cia)     error_rates.append(np.mean(pred_i != y_test))     print(f\"K={k}: Erro={error_rates[-1]:.4f}\")  # Plot do Cotovelo plt.figure(figsize=(10, 6)) plt.plot(k_values, error_rates, color='blue', linestyle='dashed', marker='o', markerfacecolor='red', markersize=10) plt.title('Taxa de Erro vs. Valor de K') plt.xlabel('K (N\u00famero de Vizinhos)') plt.ylabel('Taxa de Erro') plt.xticks(k_values) plt.show() <pre>K=1: Erro=0.0345\nK=3: Erro=0.0228\nK=5: Erro=0.0218\nK=7: Erro=0.0215\nK=9: Erro=0.0215\nK=11: Erro=0.0215\nK=13: Erro=0.0215\nK=15: Erro=0.0217\nK=17: Erro=0.0216\nK=19: Erro=0.0217\n</pre> In\u00a0[22]: Copied! <pre>best_k = 11\n</pre> best_k = 11 In\u00a0[23]: Copied! <pre>knn = KNeighborsClassifier(n_neighbors=best_k)\nknn.fit(X_train, y_train)\n</pre> knn = KNeighborsClassifier(n_neighbors=best_k) knn.fit(X_train, y_train) Out[23]: <pre>KNeighborsClassifier(n_neighbors=11)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifier?Documentation for KNeighborsClassifieriFitted Parameters n_neighbors\u00a0 11 weights\u00a0 'uniform' algorithm\u00a0 'auto' leaf_size\u00a0 30 p\u00a0 2 metric\u00a0 'minkowski' metric_params\u00a0 None n_jobs\u00a0 None In\u00a0[24]: Copied! <pre>y_pred = knn.predict(X_test)\n\nprint(f\"Acur\u00e1cia: {accuracy_score(y_test, y_pred):.4f}\")\nprint(\"\\nRelat\u00f3rio de Classifica\u00e7\u00e3o:\")\nprint(classification_report(y_test, y_pred, target_names=['Venda Normal', 'Cancelamento']))\n\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(6, 5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=['Venda', 'Cancelado'], yticklabels=['Venda', 'Cancelado'])\nplt.title(f'Matriz de Confus\u00e3o (KNN K={best_k})')\nplt.ylabel('Real')\nplt.xlabel('Predito')\nplt.show()\n</pre> y_pred = knn.predict(X_test)  print(f\"Acur\u00e1cia: {accuracy_score(y_test, y_pred):.4f}\") print(\"\\nRelat\u00f3rio de Classifica\u00e7\u00e3o:\") print(classification_report(y_test, y_pred, target_names=['Venda Normal', 'Cancelamento']))  cm = confusion_matrix(y_test, y_pred) plt.figure(figsize=(6, 5)) sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=['Venda', 'Cancelado'], yticklabels=['Venda', 'Cancelado']) plt.title(f'Matriz de Confus\u00e3o (KNN K={best_k})') plt.ylabel('Real') plt.xlabel('Predito') plt.show() <pre>Acur\u00e1cia: 0.9785\n\nRelat\u00f3rio de Classifica\u00e7\u00e3o:\n              precision    recall  f1-score   support\n\nVenda Normal       0.98      1.00      0.99    119377\nCancelamento       0.62      0.05      0.10      2672\n\n    accuracy                           0.98    122049\n   macro avg       0.80      0.53      0.54    122049\nweighted avg       0.97      0.98      0.97    122049\n\n</pre>"},{"location":"ml/online-retail/3-knn/knn/#knn","title":"KNN\u00b6","text":""},{"location":"ml/online-retail/3-knn/knn/#libs","title":"Libs\u00b6","text":""},{"location":"ml/online-retail/3-knn/knn/#carga-dos-dados-e-target","title":"Carga dos dados e Target\u00b6","text":""},{"location":"ml/online-retail/3-knn/knn/#exploracao","title":"Explora\u00e7\u00e3o\u00b6","text":""},{"location":"ml/online-retail/3-knn/knn/#pre-processamento","title":"Pr\u00e9-processamento\u00b6","text":""},{"location":"ml/online-retail/3-knn/knn/#divisao-treinoteste","title":"Divis\u00e3o treino/teste\u00b6","text":""},{"location":"ml/online-retail/3-knn/knn/#normalizacao","title":"Normaliza\u00e7\u00e3o\u00b6","text":""},{"location":"ml/online-retail/3-knn/knn/#melhor-k","title":"Melhor K\u00b6","text":""},{"location":"ml/online-retail/3-knn/knn/#melhor-modelo","title":"Melhor modelo\u00b6","text":""},{"location":"ml/online-retail/3-knn/knn/#avaliacao-do-modelo","title":"Avalia\u00e7\u00e3o do modelo\u00b6","text":""},{"location":"ml/online-retail/3-knn/knn/#conclusao-e-interpretacao-dos-resultados-knn","title":"Conclus\u00e3o e Interpreta\u00e7\u00e3o dos Resultados (KNN)\u00b6","text":"<p>Os resultados indicam um cen\u00e1rio de overfitting em rela\u00e7\u00e3o \u00e0 classe majorit\u00e1ria</p>"},{"location":"ml/online-retail/3-knn/knn/#analise-das-metricas","title":"An\u00e1lise das M\u00e9tricas\u00b6","text":"<ul> <li><p>Acur\u00e1cia (97.85%): O modelo apresentou uma acur\u00e1cia global extremamente alta. No entanto, dado que a classe de \"Venda Normal\" representa a vasta maioria dos dados, esse n\u00famero \u00e9 enganoso. O modelo atingiu esse patamar classificando quase todas as inst\u00e2ncias como \"Venda Normal\".</p> </li> <li><p>Recall de Cancelamento (0.05): Este \u00e9 o indicador mais cr\u00edtico. O modelo foi capaz de identificar apenas 5% dos cancelamentos reais. Isso demonstra que, no espa\u00e7o vetorial criado pelas vari\u00e1veis (Pre\u00e7o, Quantidade, Hora, etc.), os cancelamentos n\u00e3o formam grupos densos e isolados o suficiente para que o crit\u00e9rio de \"vizinhan\u00e7a\" do KNN funcione bem sem tratamentos adicionais.</p> </li> <li><p>Precision de Cancelamento (0.62): Apesar do baixo recall, quando o modelo identificou um cancelamento, ele estava correto em 62% das vezes. Isso sugere que ele s\u00f3 \"arrisca\" a previs\u00e3o de cancelamento em casos muito \u00f3bvios, sendo excessivamente conservador.</p> </li> </ul> <p>Para este conjunto de dados espec\u00edfico, o KNN em sua forma padr\u00e3o priorizou a especificidade (acertar as vendas normais) em detrimento da sensibilidade (encontrar cancelamentos). A alta dimensionalidade e a dispers\u00e3o dos dados de cancelamento dificultaram a forma\u00e7\u00e3o de fronteiras de decis\u00e3o claras baseadas em dist\u00e2ncia.</p>"},{"location":"ml/online-retail/4-kmeans/kmeans/","title":"K-Means Clustering: Segmenta\u00e7\u00e3o de Clientes (RFM)","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nimport datetime as dt\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.cluster import KMeans from sklearn.preprocessing import StandardScaler import datetime as dt  %matplotlib inline sns.set_style(\"whitegrid\") In\u00a0[\u00a0]: Copied! <pre>df = pd.read_csv('../assets/data.csv', encoding='ISO-8859-1')\n\ndf = df.dropna(subset=['CustomerID'])\ndf = df[(df['Quantity'] &gt; 0) &amp; (df['UnitPrice'] &gt; 0)] # Apenas compras v\u00e1lidas (sem devolu\u00e7\u00f5es ou erros)\n\n# Tratamento de Data\ndf['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n\nprint(f\"Dados limpos: {df.shape}\")\ndf.head()\n</pre> df = pd.read_csv('../assets/data.csv', encoding='ISO-8859-1')  df = df.dropna(subset=['CustomerID']) df = df[(df['Quantity'] &gt; 0) &amp; (df['UnitPrice'] &gt; 0)] # Apenas compras v\u00e1lidas (sem devolu\u00e7\u00f5es ou erros)  # Tratamento de Data df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])  print(f\"Dados limpos: {df.shape}\") df.head() In\u00a0[2]: Copied! <pre># Data de refer\u00eancia (dia seguinte \u00e0 \u00faltima compra da base)\nsnapshot_date = df['InvoiceDate'].max() + dt.timedelta(days=1)\n\n# C\u00e1lculo do Total Gasto por linha\ndf['TotalSum'] = df['Quantity'] * df['UnitPrice']\n\n# Agrupamento por Cliente (RFM)\nrfm = df.groupby(['CustomerID']).agg({\n    'InvoiceDate': lambda x: (snapshot_date - x.max()).days, # Rec\u00eancia\n    'InvoiceNo': 'nunique',                                  # Frequ\u00eancia\n    'TotalSum': 'sum'                                        # Monet\u00e1rio\n})\n\n# Renomeando colunas\nrfm.rename(columns={\n    'InvoiceDate': 'Recency',\n    'InvoiceNo': 'Frequency',\n    'TotalSum': 'Monetary'\n}, inplace=True)\n\nprint(\"Amostra da Base RFM:\")\ndisplay(rfm.head())\n</pre> # Data de refer\u00eancia (dia seguinte \u00e0 \u00faltima compra da base) snapshot_date = df['InvoiceDate'].max() + dt.timedelta(days=1)  # C\u00e1lculo do Total Gasto por linha df['TotalSum'] = df['Quantity'] * df['UnitPrice']  # Agrupamento por Cliente (RFM) rfm = df.groupby(['CustomerID']).agg({     'InvoiceDate': lambda x: (snapshot_date - x.max()).days, # Rec\u00eancia     'InvoiceNo': 'nunique',                                  # Frequ\u00eancia     'TotalSum': 'sum'                                        # Monet\u00e1rio })  # Renomeando colunas rfm.rename(columns={     'InvoiceDate': 'Recency',     'InvoiceNo': 'Frequency',     'TotalSum': 'Monetary' }, inplace=True)  print(\"Amostra da Base RFM:\") display(rfm.head()) <pre>Amostra da Base RFM:\n</pre> Recency Frequency Monetary CustomerID 12346.0 326 1 77183.60 12347.0 2 7 4310.00 12348.0 75 4 1797.24 12349.0 19 1 1757.55 12350.0 310 1 334.40 In\u00a0[3]: Copied! <pre># Log transform para lidar com outliers extremos de valor gasto\nrfm_log = np.log1p(rfm)\n\n# Normaliza\u00e7\u00e3o (StandardScaler)\nscaler = StandardScaler()\nrfm_scaled = scaler.fit_transform(rfm_log)\n\n# Transformando de volta em DataFrame para facilitar uso\nrfm_scaled_df = pd.DataFrame(rfm_scaled, index=rfm.index, columns=rfm.columns)\n</pre> # Log transform para lidar com outliers extremos de valor gasto rfm_log = np.log1p(rfm)  # Normaliza\u00e7\u00e3o (StandardScaler) scaler = StandardScaler() rfm_scaled = scaler.fit_transform(rfm_log)  # Transformando de volta em DataFrame para facilitar uso rfm_scaled_df = pd.DataFrame(rfm_scaled, index=rfm.index, columns=rfm.columns) In\u00a0[5]: Copied! <pre>inertia = []\nk_range = range(1, 11)\n\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n    kmeans.fit(rfm_scaled_df)\n    inertia.append(kmeans.inertia_)\n\nplt.figure(figsize=(10,6))\nplt.plot(k_range, inertia, marker='o', linestyle='--')\nplt.title('M\u00e9todo do Cotovelo (Elbow Curve)')\nplt.xlabel('N\u00famero de Clusters (K)')\nplt.ylabel('In\u00e9rcia')\nplt.show()\n</pre> inertia = [] k_range = range(1, 11)  for k in k_range:     kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')     kmeans.fit(rfm_scaled_df)     inertia.append(kmeans.inertia_)  plt.figure(figsize=(10,6)) plt.plot(k_range, inertia, marker='o', linestyle='--') plt.title('M\u00e9todo do Cotovelo (Elbow Curve)') plt.xlabel('N\u00famero de Clusters (K)') plt.ylabel('In\u00e9rcia') plt.show() In\u00a0[7]: Copied! <pre>k_final = 3\nkmeans = KMeans(n_clusters=k_final, random_state=42, n_init='auto')\nkmeans.fit(rfm_scaled_df)\n\n# Atribuindo os clusters aos dados originais (sem escala) para interpreta\u00e7\u00e3o\nrfm['Cluster'] = kmeans.labels_\n\nprint(f\"Clusteriza\u00e7\u00e3o conclu\u00edda com {k_final} grupos.\")\nprint(rfm['Cluster'].value_counts())\n</pre> k_final = 3 kmeans = KMeans(n_clusters=k_final, random_state=42, n_init='auto') kmeans.fit(rfm_scaled_df)  # Atribuindo os clusters aos dados originais (sem escala) para interpreta\u00e7\u00e3o rfm['Cluster'] = kmeans.labels_  print(f\"Clusteriza\u00e7\u00e3o conclu\u00edda com {k_final} grupos.\") print(rfm['Cluster'].value_counts()) <pre>Clusteriza\u00e7\u00e3o conclu\u00edda com 3 grupos.\nCluster\n2    1866\n0    1696\n1     776\nName: count, dtype: int64\n</pre> In\u00a0[8]: Copied! <pre># M\u00e9dias por Cluster\ncluster_avg = rfm.groupby('Cluster').mean()\nprint(cluster_avg)\n\n# Visualiza\u00e7\u00e3o 3D (Recency vs Frequency vs Monetary)\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection='3d')\n\n# Cores baseadas no cluster\nsc = ax.scatter(rfm_log['Recency'], rfm_log['Frequency'], rfm_log['Monetary'], \n                c=rfm['Cluster'], cmap='viridis', s=40, alpha=0.6)\n\nax.set_xlabel('Recency (Log)')\nax.set_ylabel('Frequency (Log)')\nax.set_zlabel('Monetary (Log)')\nplt.title('Distribui\u00e7\u00e3o dos Clusters (Espa\u00e7o Logar\u00edtmico)')\nplt.colorbar(sc)\nplt.show()\n</pre> # M\u00e9dias por Cluster cluster_avg = rfm.groupby('Cluster').mean() print(cluster_avg)  # Visualiza\u00e7\u00e3o 3D (Recency vs Frequency vs Monetary) fig = plt.figure(figsize=(10, 8)) ax = fig.add_subplot(111, projection='3d')  # Cores baseadas no cluster sc = ax.scatter(rfm_log['Recency'], rfm_log['Frequency'], rfm_log['Monetary'],                  c=rfm['Cluster'], cmap='viridis', s=40, alpha=0.6)  ax.set_xlabel('Recency (Log)') ax.set_ylabel('Frequency (Log)') ax.set_zlabel('Monetary (Log)') plt.title('Distribui\u00e7\u00e3o dos Clusters (Espa\u00e7o Logar\u00edtmico)') plt.colorbar(sc) plt.show() <pre>            Recency  Frequency     Monetary\nCluster                                    \n0         44.463443   3.368514  1257.688940\n1         17.070876  13.274485  7865.636662\n2        167.613076   1.349411   361.539877\n</pre>"},{"location":"ml/online-retail/4-kmeans/kmeans/#k-means-clustering-segmentacao-de-clientes-rfm","title":"K-Means Clustering: Segmenta\u00e7\u00e3o de Clientes (RFM)\u00b6","text":"<p>Diferente da abordagem supervisionada (\u00c1rvore/KNN) focada na previs\u00e3o de um r\u00f3tulo (Cancelamento), utiliza\u00e7\u00e3o de Aprendizado N\u00e3o Supervisionado para descoberta de grupos naturais de clientes.</p> <p>Aplica\u00e7\u00e3o da t\u00e9cnica RFM (Recency, Frequency, Monetary), amplamente utilizada em marketing e e-commerce para defini\u00e7\u00e3o do perfil do consumidor.</p>"},{"location":"ml/online-retail/4-kmeans/kmeans/#1-importacao-de-bibliotecas","title":"1. Importa\u00e7\u00e3o de Bibliotecas\u00b6","text":""},{"location":"ml/online-retail/4-kmeans/kmeans/#2-carga-e-preparacao-dos-dados","title":"2. Carga e Prepara\u00e7\u00e3o dos Dados\u00b6","text":"<p>Carregamento da base original e realiza\u00e7\u00e3o da limpeza inicial, com foco apenas em transa\u00e7\u00f5es v\u00e1lidas contendo identifica\u00e7\u00e3o de cliente (<code>CustomerID</code>).</p>"},{"location":"ml/online-retail/4-kmeans/kmeans/#3-engenharia-de-features-rfm","title":"3. Engenharia de Features (RFM)\u00b6","text":"<p>Para clusteriza\u00e7\u00e3o de clientes, transforma\u00e7\u00e3o dos dados transacionais (uma linha por produto) em dados de clientes (uma linha por cliente).</p> <p>Cria\u00e7\u00e3o do dataset RFM:</p> <ul> <li>Recency (Rec\u00eancia): Dias desde a \u00faltima compra.</li> <li>Frequency (Frequ\u00eancia): Quantidade de compras \u00fanicas (Invoices).</li> <li>Monetary (Monet\u00e1rio): Total gasto pelo cliente.</li> </ul>"},{"location":"ml/online-retail/4-kmeans/kmeans/#4-pre-processamento-escalonamento","title":"4. Pr\u00e9-processamento: Escalonamento\u00b6","text":"<p>Sensibilidade do K-Means \u00e0 escala. Devido \u00e0 varia\u00e7\u00e3o de 'Monetary' (milhares) e 'Frequency' (unidades), necessidade de normaliza\u00e7\u00e3o dos dados (StandardScaler) para equil\u00edbrio das dist\u00e2ncias.</p> <p>Nota: Aplica\u00e7\u00e3o de Log Transformation pr\u00e9via para redu\u00e7\u00e3o da assimetria (skewness) dos dados financeiros.</p>"},{"location":"ml/online-retail/4-kmeans/kmeans/#5-metodo-do-cotovelo-elbow-method","title":"5. M\u00e9todo do Cotovelo (Elbow Method)\u00b6","text":"<p>Teste de valores de K (n\u00famero de grupos) de 1 a 10 para identifica\u00e7\u00e3o do ponto de estabiliza\u00e7\u00e3o da in\u00e9rcia (erro).</p>"},{"location":"ml/online-retail/4-kmeans/kmeans/#6-treinamento-e-clusterizacao","title":"6. Treinamento e Clusteriza\u00e7\u00e3o\u00b6","text":"<p>Baseado na curva acima (o \"cotovelo\" geralmente aparece em K=3 ou K=4), escolha de K=3 para cria\u00e7\u00e3o de grupos interpret\u00e1veis (ex: Bronze, Prata, Ouro).</p>"},{"location":"ml/online-retail/4-kmeans/kmeans/#7-analise-e-interpretacao-dos-resultados","title":"7. An\u00e1lise e Interpreta\u00e7\u00e3o dos Resultados\u00b6","text":"<p>An\u00e1lise das m\u00e9dias de cada cluster para entendimento do perfil do cliente.</p>"},{"location":"ml/online-retail/4-kmeans/kmeans/#conclusao","title":"Conclus\u00e3o\u00b6","text":"<p>Aplica\u00e7\u00e3o do K-Means na base RFM permitindo a segmenta\u00e7\u00e3o dos clientes em 3 perfis:</p> <ol> <li>Cluster 0 (Prov\u00e1vel: \"Novos ou Baixo Valor\"): Clientes com rec\u00eancia alta ou valor gasto baixo.</li> <li>Cluster 1 (Prov\u00e1vel: \"Leais\" ou \"Intermedi\u00e1rios\"): Compram com certa frequ\u00eancia e gastam um valor m\u00e9dio.</li> <li>Cluster 2 (Prov\u00e1vel: \"VIPs\" ou \"Champions\"): Clientes que compraram recentemente, compram muito frequentemente e gastam valores altos.</li> </ol> <p>Essa segmenta\u00e7\u00e3o viabiliza estrat\u00e9gias de marketing diferenciadas, como campanhas de reten\u00e7\u00e3o para o Cluster 0 e programas de fidelidade VIP para o Cluster 2.</p>"},{"location":"ml/online-retail/5-random/random/","title":"Random Forest (Floresta Aleat\u00f3ria)","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\nfrom sklearn.preprocessing import LabelEncoder\n\n# Configura\u00e7\u00f5es visuais\nsns.set_style(\"whitegrid\")\n%matplotlib inline\n\n# 1. Carga\ndf = pd.read_csv('../assets/data.csv', encoding='ISO-8859-1')\n\n# 2. Limpeza e Target\ndf = df.dropna(subset=['CustomerID'])\ndf['Is_Cancelled'] = df['InvoiceNo'].astype(str).str.upper().str.startswith('C').astype(int)\n\n# 3. Feature Engineering\n# Datas\ndf['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\ndf['Hour'] = df['InvoiceDate'].dt.hour\ndf['Month'] = df['InvoiceDate'].dt.month\n\n# Dados Num\u00e9ricos (Evitando vazamento com abs)\ndf['Quantity_Abs'] = df['Quantity'].abs()\ndf['TotalAmount'] = df['Quantity_Abs'] * df['UnitPrice']\n\n# Encoding de Pa\u00eds\nle = LabelEncoder()\ndf['Country_Encoded'] = le.fit_transform(df['Country'])\n\n# Sele\u00e7\u00e3o Final\nfeatures = ['UnitPrice', 'Quantity_Abs', 'TotalAmount', 'Hour', 'Month', 'Country_Encoded']\nX = df[features]\ny = df['Is_Cancelled']\n\nprint(\"Dados processados. Distribui\u00e7\u00e3o das classes:\")\nprint(y.value_counts(normalize=True))\n\n# Divis\u00e3o Treino/Teste\n# Mantemos stratify=y para garantir que os cancelamentos (2%) sejam distribu\u00eddos igualmente\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\nprint(f\"Treino: {X_train.shape}\")\nprint(f\"Teste: {X_test.shape}\")\n\n# Instanciando o Random Forest\n# n_estimators=100: Criaremos 100 \u00e1rvores diferentes (padr\u00e3o da ind\u00fastria)\n# class_weight='balanced': Fundamental para dar peso maior aos cancelamentos raros\n# n_jobs=-1: Usa todos os processadores do PC para treinar mais r\u00e1pido\nrf_model = RandomForestClassifier(\n    n_estimators=100,\n    max_depth=10,       # Um pouco mais profundo que a \u00e1rvore \u00fanica, pois o ensemble controla o overfitting\n    random_state=42,\n    class_weight='balanced',\n    n_jobs=-1\n)\n\n# Treinamento\nprint(\"Treinando a Floresta Aleat\u00f3ria...\")\nrf_model.fit(X_train, y_train)\nprint(\"Modelo treinado!\")\n\n\n# Predi\u00e7\u00f5es\ny_pred = rf_model.predict(X_test)\ny_proba = rf_model.predict_proba(X_test)[:, 1] # Probabilidade para Curva ROC\n\n# M\u00e9tricas\nprint(\"--- Relat\u00f3rio de Classifica\u00e7\u00e3o Random Forest ---\")\nprint(classification_report(y_test, y_pred, target_names=['Venda Normal', 'Cancelamento']))\n\n# Matriz de Confus\u00e3o\nplt.figure(figsize=(6, 5))\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Oranges', xticklabels=['Venda', 'Cancelado'], yticklabels=['Venda', 'Cancelado'])\nplt.title('Matriz de Confus\u00e3o: Random Forest')\nplt.ylabel('Real')\nplt.xlabel('Predito')\nplt.show()\n\n\n# Extraindo a import\u00e2ncia\nfeature_importance = pd.DataFrame({\n    'Feature': features,\n    'Importancia': rf_model.feature_importances_\n}).sort_values(by='Importancia', ascending=False)\n\n# Visualizando\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Importancia', y='Feature', data=feature_importance, palette='viridis')\nplt.title('Quais vari\u00e1veis mais impactam no Cancelamento?')\nplt.xlabel('Grau de Import\u00e2ncia (0 a 1)')\nplt.show()\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve from sklearn.preprocessing import LabelEncoder  # Configura\u00e7\u00f5es visuais sns.set_style(\"whitegrid\") %matplotlib inline  # 1. Carga df = pd.read_csv('../assets/data.csv', encoding='ISO-8859-1')  # 2. Limpeza e Target df = df.dropna(subset=['CustomerID']) df['Is_Cancelled'] = df['InvoiceNo'].astype(str).str.upper().str.startswith('C').astype(int)  # 3. Feature Engineering # Datas df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate']) df['Hour'] = df['InvoiceDate'].dt.hour df['Month'] = df['InvoiceDate'].dt.month  # Dados Num\u00e9ricos (Evitando vazamento com abs) df['Quantity_Abs'] = df['Quantity'].abs() df['TotalAmount'] = df['Quantity_Abs'] * df['UnitPrice']  # Encoding de Pa\u00eds le = LabelEncoder() df['Country_Encoded'] = le.fit_transform(df['Country'])  # Sele\u00e7\u00e3o Final features = ['UnitPrice', 'Quantity_Abs', 'TotalAmount', 'Hour', 'Month', 'Country_Encoded'] X = df[features] y = df['Is_Cancelled']  print(\"Dados processados. Distribui\u00e7\u00e3o das classes:\") print(y.value_counts(normalize=True))  # Divis\u00e3o Treino/Teste # Mantemos stratify=y para garantir que os cancelamentos (2%) sejam distribu\u00eddos igualmente X_train, X_test, y_train, y_test = train_test_split(     X, y, test_size=0.3, random_state=42, stratify=y )  print(f\"Treino: {X_train.shape}\") print(f\"Teste: {X_test.shape}\")  # Instanciando o Random Forest # n_estimators=100: Criaremos 100 \u00e1rvores diferentes (padr\u00e3o da ind\u00fastria) # class_weight='balanced': Fundamental para dar peso maior aos cancelamentos raros # n_jobs=-1: Usa todos os processadores do PC para treinar mais r\u00e1pido rf_model = RandomForestClassifier(     n_estimators=100,     max_depth=10,       # Um pouco mais profundo que a \u00e1rvore \u00fanica, pois o ensemble controla o overfitting     random_state=42,     class_weight='balanced',     n_jobs=-1 )  # Treinamento print(\"Treinando a Floresta Aleat\u00f3ria...\") rf_model.fit(X_train, y_train) print(\"Modelo treinado!\")   # Predi\u00e7\u00f5es y_pred = rf_model.predict(X_test) y_proba = rf_model.predict_proba(X_test)[:, 1] # Probabilidade para Curva ROC  # M\u00e9tricas print(\"--- Relat\u00f3rio de Classifica\u00e7\u00e3o Random Forest ---\") print(classification_report(y_test, y_pred, target_names=['Venda Normal', 'Cancelamento']))  # Matriz de Confus\u00e3o plt.figure(figsize=(6, 5)) cm = confusion_matrix(y_test, y_pred) sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges', xticklabels=['Venda', 'Cancelado'], yticklabels=['Venda', 'Cancelado']) plt.title('Matriz de Confus\u00e3o: Random Forest') plt.ylabel('Real') plt.xlabel('Predito') plt.show()   # Extraindo a import\u00e2ncia feature_importance = pd.DataFrame({     'Feature': features,     'Importancia': rf_model.feature_importances_ }).sort_values(by='Importancia', ascending=False)  # Visualizando plt.figure(figsize=(10, 6)) sns.barplot(x='Importancia', y='Feature', data=feature_importance, palette='viridis') plt.title('Quais vari\u00e1veis mais impactam no Cancelamento?') plt.xlabel('Grau de Import\u00e2ncia (0 a 1)') plt.show() <pre>Dados processados. Distribui\u00e7\u00e3o das classes:\nIs_Cancelled\n0    0.978111\n1    0.021889\nName: proportion, dtype: float64\nTreino: (284780, 6)\nTeste: (122049, 6)\nTreinando a Floresta Aleat\u00f3ria...\nModelo treinado!\n--- Relat\u00f3rio de Classifica\u00e7\u00e3o Random Forest ---\n              precision    recall  f1-score   support\n\nVenda Normal       0.99      0.81      0.89    119377\nCancelamento       0.07      0.59      0.12      2672\n\n    accuracy                           0.80    122049\n   macro avg       0.53      0.70      0.50    122049\nweighted avg       0.97      0.80      0.87    122049\n\n</pre> <pre>/tmp/ipykernel_93204/67909455.py:96: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.barplot(x='Importancia', y='Feature', data=feature_importance, palette='viridis')\n</pre>"},{"location":"ml/online-retail/5-random/random/#random-forest-floresta-aleatoria","title":"Random Forest (Floresta Aleat\u00f3ria)\u00b6","text":""},{"location":"ml/online-retail/5-random/random/#o-que-e","title":"O que \u00e9?\u00b6","text":"<p>O Random Forest \u00e9 um algoritmo de Ensemble Learning (Aprendizado em Conjunto) que cria uma \"floresta\" de v\u00e1rias \u00c1rvores de Decis\u00e3o durante o treinamento.</p>"},{"location":"ml/online-retail/5-random/random/#principais-diferenciais-baseado-no-material-de-apoio","title":"Principais Diferenciais (Baseado no Material de Apoio):\u00b6","text":"<ol> <li>Bagging (Bootstrap Aggregating): Cada \u00e1rvore \u00e9 treinada com uma amostra aleat\u00f3ria dos dados (com reposi\u00e7\u00e3o), o que reduz a vari\u00e2ncia do modelo e evita o overfitting (o grande problema de uma \u00fanica \u00e1rvore).</li> <li>Sele\u00e7\u00e3o Aleat\u00f3ria de Features: Em cada divis\u00e3o de n\u00f3, o algoritmo considera apenas um subconjunto aleat\u00f3rio de colunas, garantindo que as \u00e1rvores sejam diferentes umas das outras (decorrela\u00e7\u00e3o).</li> <li>Vota\u00e7\u00e3o: Para classificar, o modelo pega a \"vota\u00e7\u00e3o\" de todas as \u00e1rvores e escolhe a classe vencedora (Moda).</li> </ol>"},{"location":"ml/online-retail/5-random/random/#objetivo-do-experimento","title":"Objetivo do Experimento\u00b6","text":"<p>Utilizar o Random Forest para superar os resultados obtidos com a Decision Tree simples na detec\u00e7\u00e3o de Risco de Cancelamento, visando melhorar o equil\u00edbrio entre Precis\u00e3o e Recall.</p>"},{"location":"ml/online-retail/5-random/random/#7-analise-e-interpretacao-dos-resultados-conclusao","title":"7. An\u00e1lise e Interpreta\u00e7\u00e3o dos Resultados (Conclus\u00e3o)\u00b6","text":"<p>Os resultados do Random Forest revelam uma estrat\u00e9gia de classifica\u00e7\u00e3o agressiva, oposta \u00e0 passividade observada no KNN e similar (por\u00e9m mais est\u00e1vel) \u00e0 \u00c1rvore de Decis\u00e3o simples.</p>"},{"location":"ml/online-retail/5-random/random/#interpretacao-das-metricas","title":"Interpreta\u00e7\u00e3o das M\u00e9tricas\u00b6","text":"<ul> <li><p>Acur\u00e1cia Global (80%): Diferente do KNN que teve 98% de acur\u00e1cia prevendo apenas a classe majorit\u00e1ria, o Random Forest caiu para 80%. Isso n\u00e3o \u00e9 um erro, mas uma escolha deliberada do algoritmo. Ao usar <code>class_weight='balanced'</code>, o modelo sacrificou a acur\u00e1cia global (errando mais nas Vendas Normais) para tentar capturar os Cancelamentos. Ele est\u00e1 disposto a errar 20% das vendas boas para n\u00e3o perder os riscos.</p> </li> <li><p>Recall de Cancelamento (0.59): O modelo detectou corretamente 59% de todos os cancelamentos reais. Este resultado \u00e9 significativamente superior aos 5% do KNN. Para um sistema de detec\u00e7\u00e3o de fraudes ou riscos, este \u00e9 o indicador de sucesso: conseguimos \"pescar\" mais da metade dos problemas.</p> </li> <li><p>Precision de Cancelamento (0.07): O custo do alto recall foi a baixa precis\u00e3o. De todas as vezes que o modelo gritou \"Cancelamento!\", ele s\u00f3 acertou em 7% dos casos. Isso significa que ele gerou um volume muito alto de Falsos Positivos. O modelo est\u00e1 agindo como um \"alarme sens\u00edvel\": dispara por qualquer suspeita.</p> </li> </ul> <p>Embora a precis\u00e3o de 7% pare\u00e7a baixa, em um fluxo de neg\u00f3cio, \u00e9 mais seguro revisar manualmente transa\u00e7\u00f5es suspeitas (mesmo que muitas sejam leg\u00edtimas) do que deixar passar 95% dos cancelamentos invis\u00edveis, como ocorreu no KNN. O modelo cumpriu o objetivo de priorizar a classe minorit\u00e1ria.</p>"}]}